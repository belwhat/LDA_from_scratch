{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation by Valentin TASSEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet allocation (LDA) is a generative probabilistic model for collections of discret data such as text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find short descriptions of the members of a collection that enable efficient\n",
    "processing of large collections while preserving the essential statistical relationships that are useful\n",
    "for basic tasks such as classification, novelty detection, summarization, and similarity and relevance\n",
    "judgments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Notation and terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We define the following terms :\n",
    "- A <i>word</i> is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1,...,V\\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the vth word in the vocabulary is represented by a $V$-vector $w$  such that $w^v = 1$ and $w^u = 0$ for $u \\neq v$.\n",
    "- A <i>document</i> is a sequence of $N$ words denoted by $w = (w_1, w_2,...,w_N)$, where $w_n$ is the $n$th word in the sequence\n",
    "- A <i>corpus</i> is a collection of $M$ documents denoted by $D = \\{w_1,w_2,...,w_M\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the distributions :\n",
    "- Distribution of topics over the documents, we call it $\\theta$. Specifically $\\theta_i$ is the topic distribution for document $i$.\n",
    "\n",
    "- We introduce $\\varphi_k$ as the word distribution for topic $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Gibbs sampling to find $\\theta$ and $\\varphi$ using the 20 newsgroup dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importation of library / definition of the vocabulary</h3>\n",
    "<br>\n",
    "We use a Reddit News dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/RedditNews.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x[1] for x in data]\n",
    "corpus = corpus[:n_samples] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary fitted on the news corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=10000,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news': 5392,\n",
       " '117': 26,\n",
       " 'year': 8821,\n",
       " 'old': 5540,\n",
       " 'woman': 8759,\n",
       " 'mexico': 5070,\n",
       " 'city': 1637,\n",
       " 'finally': 3255,\n",
       " 'received': 6483,\n",
       " 'birth': 1069}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(vocabulary.items())[0:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for row in tf.toarray():\n",
    "    present_words = np.where(row != 0)[0].tolist()\n",
    "    present_words_with_count = []\n",
    "    for word_idx in present_words:\n",
    "        for count in range(row[word_idx]):\n",
    "            present_words_with_count.append(word_idx)\n",
    "    docs.append(present_words_with_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implement LDA with Gibbs sampling</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- D : Number of documents\n",
    "- V : Size of the vocabulary\n",
    "- T : Number of topics\n",
    "<br>\n",
    "<br>\n",
    "Parameters\n",
    "- Beta : The parameter of the Dirichlet prior on the per-document topic distributions\n",
    "- Alpha : The parameter of the Dirichlet prior on the per-topic word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(docs)\n",
    "V = len(vocabulary)\n",
    "T = 10\n",
    "\n",
    "beta = 1/T\n",
    "alpha = 1/T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the sampling of a new topic $z_{ij}$ for a word $w_{ij}$ by the following formula :\n",
    "<br>\n",
    "$P(z_{ij}|z_{kl}\\ with\\ k \\neq i\\  and\\  l \\neq j, w) = \\frac{\\theta_{ik} \\alpha}{N_i + \\alpha T} \\frac{\\Phi_kw + \\beta}{\\sum_{w\\in V}\\Phi_{kw} + \\beta V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_d_n = [[0 for _ in range(len(d)) ] for d in docs] #z_i_j Topic z of a word\n",
    "theta_d_z = np.zeros((D, T)) #Distribution of topics over the documents\n",
    "phi_z_w = np.zeros((T, V)) #Distribution of word over topics\n",
    "n_d = np.zeros((D))\n",
    "n_z = np.zeros((T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tobias Sterbak - Latent Dirichlet ALlocation \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
